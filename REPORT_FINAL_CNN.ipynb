{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fCxxHTuL-tz3",
        "outputId": "3c50009b-5b1d-454e-c1f7-74649277a102"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-907070862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScEQ8JFz-v09"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tvJplIO-v35"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vT5uOpU-v66"
      },
      "outputs": [],
      "source": [
        "CLASSES = {\n",
        "    \"healthy\": 0,\n",
        "    # \"brb\": 1,\n",
        "    \"ub\": 1,\n",
        "    \"brb_lite\": 2,\n",
        "    \"gi\": 3,\n",
        "    \"sitsc\": 4\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJlubyOm-v-C"
      },
      "outputs": [],
      "source": [
        "BASE = \"/content/drive/MyDrive/dataset4\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlUMAJw5B0rB"
      },
      "outputs": [],
      "source": [
        "def collect_pairs(base, cls):\n",
        "    img_folder = os.path.join(base, f\"{cls}_segments\")\n",
        "    feat_folder = os.path.join(base, f\"{cls}_features\")\n",
        "\n",
        "    if not os.path.exists(img_folder) or not os.path.exists(feat_folder):\n",
        "        print(f\"[WARN] Missing folder for {cls}\")\n",
        "        return []\n",
        "\n",
        "    img_files = {f.split('.')[0]: f for f in os.listdir(img_folder) if f.endswith(\".png\")}\n",
        "    feat_files = {f.split('.')[0]: f for f in os.listdir(feat_folder) if f.endswith(\".csv\")}\n",
        "\n",
        "    pairs = []\n",
        "    for key in img_files:\n",
        "        if key in feat_files:\n",
        "            pairs.append((\n",
        "                os.path.join(img_folder, img_files[key]),\n",
        "                os.path.join(feat_folder, feat_files[key]),\n",
        "                CLASSES[cls]\n",
        "            ))\n",
        "\n",
        "    return pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxXJrhbRB0uK"
      },
      "outputs": [],
      "source": [
        "all_samples = []\n",
        "\n",
        "for cls in CLASSES:\n",
        "    pairs = collect_pairs(BASE, cls)\n",
        "    print(cls, \"â†’\", len(pairs), \"samples\")\n",
        "    all_samples.extend(pairs)\n",
        "\n",
        "df = pd.DataFrame(all_samples, columns=[\"img_path\", \"feat_path\", \"label\"])\n",
        "print(\"\\nLabel counts:\")\n",
        "print(df[\"label\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBIYa6_rC2Wj"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 64\n",
        "\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = img.astype(\"float32\") / 255.0\n",
        "    return img\n",
        "\n",
        "def load_features(path):\n",
        "    arr = np.loadtxt(path, delimiter=\",\")\n",
        "    return arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev61QYM_B0xH"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from joblib import Parallel, delayed  # parallel loading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly6P4BUzB00c"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 64\n",
        "\n",
        "def fast_load_sample(img_path, feat_path, label):\n",
        "    # load img\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = img.astype(\"float32\") / 255.0\n",
        "\n",
        "    # load numeric features (very fast)\n",
        "    feat = np.loadtxt(feat_path, delimiter=\",\")\n",
        "\n",
        "    return img, feat, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYaCio9JB03r"
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "\n",
        "all_imgs = []\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "\n",
        "results = Parallel(n_jobs=8, backend=\"threading\")(\n",
        "    delayed(fast_load_sample)(row.img_path, row.feat_path, row.label)\n",
        "    for row in df.itertuples()\n",
        ")\n",
        "\n",
        "for img, feat, label in results:\n",
        "    all_imgs.append(img)\n",
        "    all_feats.append(feat)\n",
        "    all_labels.append(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2w29KAqF4O7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Ensure features is a 2D NumPy array, assuming 'all_feats' is the source list of feature arrays\n",
        "features = np.array(all_feats)\n",
        "\n",
        "# Handle infinite values by replacing them with NaN\n",
        "features[np.isinf(features)] = np.nan\n",
        "\n",
        "# Impute NaN values (which now include the original infinite values)\n",
        "# with the mean of each column. A SimpleImputer is used for this.\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "features_imputed = imputer.fit_transform(features)\n",
        "\n",
        "# Scale the imputed features\n",
        "scaler = StandardScaler()\n",
        "features = scaler.fit_transform(features_imputed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opkTsI4ZF4Li"
      },
      "outputs": [],
      "source": [
        "X_img_train, X_img_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
        "    all_imgs, features, all_labels, test_size=0.2, stratify=all_labels, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2wNvlBvF4IR"
      },
      "outputs": [],
      "source": [
        "img_in = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "x = Conv2D(32, (3,3), activation='relu')(img_in)\n",
        "x = MaxPooling2D()(x)\n",
        "\n",
        "x = Conv2D(64, (3,3), activation='relu')(x)\n",
        "x = MaxPooling2D()(x)\n",
        "\n",
        "x = Conv2D(128, (3,3), activation='relu')(x)\n",
        "x = GlobalAveragePooling2D()(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NxMT4NkF3--"
      },
      "outputs": [],
      "source": [
        "num_in = Input(shape=(features.shape[1],))\n",
        "y = Dense(64, activation='relu')(num_in)\n",
        "y = Dense(32, activation='relu')(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqTlRiZdF37j"
      },
      "outputs": [],
      "source": [
        "combined = concatenate([x, y])\n",
        "z = Dense(64, activation='relu')(combined)\n",
        "z = Dense(len(CLASSES), activation='softmax')(z)\n",
        "\n",
        "model = Model(inputs=[img_in, num_in], outputs=z)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY-GMYrOF34J"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    [np.array(X_img_train), X_num_train],\n",
        "    np.array(y_train),\n",
        "    validation_split=0.2,\n",
        "    epochs=5,\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8ME0-5cB06o"
      },
      "outputs": [],
      "source": [
        "test_pred = model.predict([np.array(X_img_test), X_num_test])\n",
        "test_pred = np.argmax(test_pred, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, test_pred)\n",
        "print(\"Test Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE9h5FKuB09-"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, test_pred)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
        "            xticklabels=CLASSES.keys(),\n",
        "            yticklabels=CLASSES.keys(),\n",
        "            cmap=\"Blues\")\n",
        "plt.title(\"CNN Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDPlH3dV-wBJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history[\"loss\"], label=\"train\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"val\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}